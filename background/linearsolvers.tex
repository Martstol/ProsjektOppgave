\section*{Linear Solvers}

TODO: Rewrite this section

A system of linear equations with $N$ unknowns can be written as 

$$Ax = b$$

where $A$ is a $N \times N$ matrix, and $x$ and $b$ are column vectors with $N$ 
elements each. We want to solve the system either directly by find an expression 
for $x$ by calculating $\inv{A}$. The solution is written as 

$$x = \inv{A}b$$

Or indirectly through some other method.

\subsection*{Direct Methods}

Direct methods are methods that solve the system of linear equations in a finite
number of steps. The conceptually simplest direct method is to find $\inv{A}$. 
There are different methods for finding the inverse of $A$, two of the most 
well known are Gaussian elimination, also known as Gauss-Jordan elimination, 
and LU decomposition, also known as LU factorization.

\subsubsection*{Gaussian elimination}

Gaussian elimination solves the system by performing row operations on the matrix 
$A$, to turn $A$ into the identity matrix. If these row operations are also performed 
on the vector $b$ then $b$ will be the solution vector when the Gaussian elimination 
is complete.

This method does not require you directly find $\inv{A}$, but finds the inverse 
indirectly. The combination of performing all the row operations is the same as 
multiplying with $\inv{A}$.

Gaussian elimination has O($n^3$) computational complexity, which means using 
Gaussian elimination is not feasible for solving equations with more than a few 
thousand unknowns with today's computing power.

Another problem with Gaussian elimination is that it is numerically unstable. 
When eliminating non-zero elements the algorithm may have to divide all the elements 
in a row with the value of the first coefficient on that row. If this number is 
close to zero any rounding error would be amplified.

\subsubsection*{LU decomposition}

The LU in LU decomposition stands for "Lower Upper". It works by factoring the 
matrix into the product of a lower and an upper triangular matrix. 

$$A = LU$$

To solve the linear system 

$$LUx = b$$

We first solve 

$$Ly = b$$

for $y$ by forward substitution. Then we solve 

$$Ux = y$$

for $x$ by backward substitution.

LU decomposition also has computational complexity O($n^3$). However LU decomposition 
is numerically stable, making it more suitable for a computer implementation.

\subsection*{Iterative Methods}

Iterative methods often start with a guess as to what the correct solution is, 
with the zero vector being a common choice for initial solution. Then they improve 
this solution by repeating a procedure multiple times, with each step 
giving an improved approximation to the solution of the problem. Iterative methods 
are not guaranteed to produce a correct solution in a given amount of steps. 

Iterative methods stop when a stopping criteria is met, normally when the difference 
between the right-hand side and the left-hand side or the change in the approximate 
solution is below a tolerance level given by the user. 

\subsubsection*{Jacobi Method}

The Jacobi method works by decomposing the matrix $A$ into a diagonal matrix $D$ 
and the matices $L$ and $U$. 

$$A = L+D+U$$

The matrix $D$ is diagonal and contains all the elements on $A$'s diagonal. $L$ is lower triangular and contains all the elements from $A$ below the diagonal. $U$ is upper triangular and contains all the elements from $A$ above the diagonal.

$$ big( L+D+U big) x = b $$

The system is then solved iteratively by computing 

$$x^{(k+1)} = \inv{D}(b - (L+U)x^{(k)})$$

To converge this method requires the system to be strictly diagonally dominant[source],
meaning that for each row, the absolute value of the diagonal element must be
greater than or equal to the sum of the absolute value of the rest of the
elements on that row.

$$|a_{ii}| > \sum_{j \neq i} |a_{ij}|$$

TODO: Parallelization

\subsubsection*{Gauss-Seidel Method}

Gauss-Seidel works similarly to the Jacobi method. The main difference is how the matrix $A$ is decomposed and the consequences of this. Gauss-Seidel decompose $A$ into a matrix 

\subsubsection*{Successive Over-relaxation}

Describe successive over-relaxation
