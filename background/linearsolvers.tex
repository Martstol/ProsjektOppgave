\section*{Linear Solvers}

TODO: Rewrite this section

A system of linear equations with $N$ unknowns can be written as 

$$Ax = b$$

where $A$ is a $N \times N$ matrix, and $x$ and $b$ are column vectors with $N$ 
elements each. We want to solve the system either directly by find an expression 
for $x$ by calculating $\inv{A}$. The solution is written as 

$$x = \inv{A}b$$

Or indirectly through some other method.

\subsection*{Direct Methods}

Direct methods are methods that solve the system of linear equations in a finite
number of steps. The conceptually simplest direct method is to find $\inv{A}$. 
There are different methods for finding the inverse of $A$, two of the most 
well known are Gaussian elimination, also known as Gauss-Jordan elimination, 
and LU decomposition, also known as LU factorization.

\subsubsection*{Gaussian elimination}

Gaussian elimination solves the system by performing row operations on the matrix 
$A$, to turn $A$ into the identity matrix. If these row operations are also performed 
on the vector $b$ then $b$ will be the solution vector when the Gaussian elimination 
is complete.

This method does not require you directly find $\inv{A}$, but finds the inverse 
indirectly. The combination of performing all the row operations is the same as 
multiplying with $\inv{A}$.

Gaussian elimination has O($n^3$) computational complexity, which means using 
Gaussian elimination is not feasible for solving equations with more than a few 
thousand unknowns with today's computing power.

Another problem with Gaussian elimination is that it is numerically unstable. 
When eliminating non-zero elements the algorithm may have to divide all the elements 
in a row with the value of the first coefficient on that row. If this number is 
close to zero any rounding error would be amplified.

\subsubsection*{LU decomposition}

The LU in LU decomposition stands for "Lower Upper". It works by factoring the 
matrix into the product of a lower and an upper triangular matrix. 

$$A = LU$$

To solve the linear system 

$$LUx = b$$

We first solve 

$$Ly = b$$

for $y$ by forward substitution. Then we solve 

$$Ux = y$$

for $x$ by backward substitution.

LU decomposition also has computational complexity O($n^3$). However LU decomposition 
is numerically stable, making it more suitable for a computer implementation.

\subsection*{Iterative Methods}

Iterative methods often start with a guess as to what the correct solution is, 
with the zero vector being a common choice for initial solution. Then they improve 
this solution by repeating a procedure multiple times, with each step 
giving an improved approximation to the solution of the problem. Iterative methods 
are not guaranteed to produce a correct solution in a given amount of steps. 

Iterative methods stop when a stopping criteria is met, normally when the difference 
between the right-hand side and the left-hand side or the change in the approximate 
solution is below a tolerance level given by the user. 

Another advantage of iterative methods is that we don't have to store the matrix 
$A$ because iterative methods don't depend on computing $\inv{A}$. We can instead 
write a specialized functions for each specific problem. This lowers the number of 
memory accesses that has to be performed. 

\subsubsection*{Jacobi Method}

The Jacobi method works by decomposing the matrix $A$ into a diagonal matrix $D$ 
and the lower and upper triangular matrices $L$ and $U$. 

$$A = L+D+U$$

The matrix $D$ is diagonal and contains all the elements on $A$'s diagonal. $L$ 
is lower triangular and contains all the elements from $A$ below the diagonal. 
$U$ is upper triangular and contains all the elements from $A$ above the diagonal.

$$(L+D+U)x = b$$

The equation is then rearranged 

$$Dx = (b - (L+U)x)$$
$$x = \inv{D}(b - (L+U)x)$$

and the system is then solved iteratively by computing 

$$x^{(k+1)} = \inv{D}(b - (L+U)x^{(k)})$$

When parallelizing the Jacobi method we first write the element-based version:

$$ x_i^{(k+1)} = \frac{1}{a_{ii}} \Big( b_i - \sum_{j \neq j} a_{ij} x_i^{(k)} \Big), i = 1, 2, \ldots, n $$

Because the new value of each $x_i$ is independent of all the other values in the 
$x$ vector, the Jacobi method is trivially parallel.

To converge this method requires the system to be strictly diagonally dominant[source],
meaning that for each row, the absolute value of the diagonal element must be
greater than or equal to the sum of the absolute value of the rest of the
elements on that row.

$$|a_{ii}| > \sum_{j \neq i} |a_{ij}|$$

The Jacobi method may converge even if this criteria is not satisfied, which is the case with the Poisson equation.

\subsubsection*{Gauss-Seidel Method}

Gauss-Seidel works similarly to the Jacobi method, by decomposing $A$ into $D$, 
$L$ and $U$. The main difference is how the equation is rearranged and the consequences 
when parallelizing the iterative solver. 

$$(L+D)x = (b - Ux)$$
$$x = \inv{(L+D)}(b - Ux)$$

$$x^{(k+1)} = \inv{(L+D)}(b - Ux^{(k)})$$

The value of $x^{(k+1)}$ has to be computed sequentially using forward substitution. 
There is no room for parallelization. Written in element-based form:

$$ x_i^{(k+1)} = \frac{1}{a_{ii}} \Big( b_i - \sum_{j < i} a_{ij} x_j^{(k+1)} - \sum_{j > i} a_{ij} x_j^{(k)} \Big),  i = 1, 2, \ldots, n $$

As we see from the first sum, the new value of $x_i$ depends on the new value of 
all previous elements in the $x$ vector.

TODO: Red-Black Ordering

The convergence of the Gauss-Seidel method requires the matrix $A$ to be either symmetric positive definite, or strictly diagonal dominant[SOURCE]. This method converges faster than the Jacobi method, because it uses the new estimates for $x$ found so far, in updating the value of the next element in the $x$ vector.

\subsubsection*{Successive Over-relaxation}

Describe successive over-relaxation
