\section*{Linear Solvers}

A system of linear equations with $N$ unknowns can be written as 

$$Ax = b$$

where $A$ is a $N \times N$ matrix, and $x$ and $b$ are column vectors with $N$ 
elements each. $A$ and $b$ are known before hand, and we want to solve for $x$.

This can either be done with direct methods which compute the solution in a finite 
number of steps or by using an iterative method which iteratively improves an initial 
guess until we are satisfied. 

\subsection*{Direct Methods}

Direct methods are methods that solve the system of linear equations in a finite
number of steps. The conceptually simplest direct method is to find $\inv{A}$. 
There are different methods for finding the inverse of $A$, two of the most 
well known are Gaussian elimination, and LU factorization.

\subsubsection*{Gaussian elimination}

Gaussian elimination solves the system by performing row operations on the matrix 
$A$, to turn $A$ into the identity matrix. If these row operations are also performed 
on the vector $b$ then $b$ will be the solution vector when the Gaussian elimination 
is complete.

This method does not require you directly find $\inv{A}$, but finds the inverse 
indirectly. The combination of performing all the row operations is the same as 
multiplying with $\inv{A}$.

Gaussian elimination has O($n^3$) computational complexity, which means using 
Gaussian elimination is not feasible for solving equations with more than a few 
thousand unknowns with today's computing power.

Another problem with Gaussian elimination is that it is numerically unstable[SOURCE]. 
When eliminating non-zero elements the algorithm may have to divide all the elements 
in a row with the value of the first coefficient on that row. If this number is 
close to zero any rounding error would be amplified.

\subsubsection*{LU factorization}

The LU in LU factorization stands for "Lower Upper". It works by factoring the 
matrix into the product of a lower and an upper triangular matrix. 

$$A = LU$$

To solve the linear system 

$$LUx = b$$

We first solve 

$$Ly = b$$

for $y$ by forward substitution. Then we solve 

$$Ux = y$$

for $x$ by backward substitution.

LU factorization also has computational complexity O($n^3$). However LU factorization 
is numerically stable[SOURCE], making it more suitable for a computer implementation.

\subsection*{Iterative Methods}

Iterative methods often start with a guess as to what the correct solution is, 
with the zero vector being a common choice for initial solution. Then they improve 
this solution by repeating a procedure multiple times, with each step 
giving an improved approximation to the solution of the problem. Iterative methods 
are not guaranteed to produce a correct solution in a given amount of steps. 

Iterative methods stop when a stopping criteria is met, normally when the difference 
between the right-hand side and the left-hand side or the change in the approximate 
solution is below a tolerance level given by the user. 

Another advantage of iterative methods is that we don't have to store the matrix 
$A$ because iterative methods don't depend on computing $\inv{A}$. This lowers 
the number of memory accesses that has to be performed. 

\subsubsection*{Jacobi Method}

The Jacobi method works by splitting the matrix $A$ into a diagonal matrix $D$ 
and the lower and upper triangular matrices $L$ and $U$. 

$$A = L+D+U$$

The matrix $D$ is diagonal and contains all the elements on $A$'s diagonal. $L$ 
is lower triangular and contains all the elements from $A$ below the diagonal. 
$U$ is upper triangular and contains all the elements from $A$ above the diagonal.

$$(L+D+U)x = b$$

The equation is then rearranged 

$$Dx = (b - (L+U)x)$$
$$x = \inv{D}(b - (L+U)x)$$

and the system is then solved iteratively by computing 

$$x^{(k+1)} = \inv{D}(b - (L+U)x^{(k)})$$

When parallelizing the Jacobi method we first write the element-based version:

$$ x_i^{(k+1)} = \frac{1}{a_{ii}} \Big( b_i - \sum_{j \neq j} a_{ij} x_i^{(k)} \Big), ~ i = 1, 2, \ldots, n $$

Because the new value of each $x_i$ is independent of the new value for all the 
other elements in the $x$ vector, the Jacobi method is trivially parallel.

This method converges for every initial guess, if and only if the spectral
radius of $I - A$ is less than 1\cite{Kreyszig}. Spectral radius is defined as 
$\rho = max_i(| \lambda_i |)$, where $\lambda_i$ is the $i$th eigen value of the
matrix[SOURCE].

\subsubsection*{Gauss-Seidel Method}

Gauss-Seidel works similarly to the Jacobi method, by splitting $A$ into $D$, 
$L$ and $U$. The main difference is how the equation is rearranged and the consequences 
when parallelizing the iterative solver. 

$$ (L+D)x = b - Ux $$
$$ (L+D)x^{(k+1)} = b - Ux^{(k)} $$
$$ Dx^{(k+1)} = b - Lx^{(k+1)} - Ux^{(k)} $$
$$ x^{(k+1)} = \inv{D}(b - Lx^{(k+1)} - Ux^{(k)}) $$

The value of $x^{(k+1)}$ has to be computed sequentially using forward substitution. 
There is no room for parallelization. Written in element-based form:

$$ x_i^{(k+1)} = \frac{1}{a_{ii}} \Big( b_i - \sum_{j < i} a_{ij} x_j^{(k+1)} 
- \sum_{j > i} a_{ij} x_j^{(k)} \Big), ~ i = 1, 2, \ldots, n $$

As we see from the first sum, the new value of $x_i$ depends on the new value of 
all previous elements in the $x$ vector.

It is possible to create a parallel implementation of Gauss-Seidel by changing
the order in which you calculate new $x$ values. This is known as Red-Black
ordering. The new ordering depends on the linear system you want to solve.

The convergence requirement for Gauss-Seidel is the same as for Jacobi.

\subsubsection*{Successive Overrelaxation}

Successive overrelaxation (SOR) is a modified version of Gauss-Seidel with the
goal of having faster convergence\cite{Kreyszig}. This method differs from Gauss-Seidel by
introducing a constant $\omega > 1$, which is called the \emph{overrelaxation
factor}. To get the equation for SOR we first add and subtract $x^{(k)}$ on the right 
hand side of Gauss-Seidel.

$$ Dx^{(k+1)} = x^{(k)} + b - Lx^{(k+1)} - (U + I)x^{(k)} $$

Then we introduce the overrelaxation factor

$$ Dx^{(k+1)} = x^{(k)} + \omega \big( b - Lx^{(k+1)} - (U + I)x^{(k)} \big) $$

A recommended value for the overrelaxation factor is $\omega = \frac{2}{1 +
\sqrt{1 - \rho}} $, where $\rho$ is the spectral radius of $-\inv{(I + L)}U$. 
