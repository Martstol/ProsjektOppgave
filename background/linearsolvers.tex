\section{Linear Solvers}

A system of linear equations with $N$ unknowns can be written as 

$$Ax = b$$

where $A$ is a $N \times N$ matrix, and $x$ and $b$ are column vectors with $N$ 
elements each. $A$ and $b$ are known before hand, and we want to solve for $x$.

This can either be done with direct methods which compute the solution in a finite 
number of steps or by using an iterative method which iteratively improves an initial 
guess until we reach convergence. 

\subsection{Direct Methods}

Direct methods are methods that solve the system of linear equations in a finite
number of steps. Two well known methods are Gaussian elimination, and LU factorization.

\subsubsection{Gaussian elimination}

Gaussian elimination solves the system by performing row operations on the matrix 
$A$, to turn $A$ into the identity matrix. If these row operations are also performed 
on the vector $b$ then $b$ will be the solution vector when the Gaussian elimination 
is complete.

Gaussian elimination has O($n^3$) computational complexity, which means using 
Gaussian elimination is not feasible for solving equations with more than a few 
thousand unknowns with today's computing power.

\subsubsection{LU factorization}

The LU in LU factorization stands for "Lower Upper". It works by factoring the 
matrix into the product of a lower and an upper triangular matrix. 

$$A = LU$$

To solve the linear system 

$$LUx = b$$

We first solve 

$$Ly = b$$

for $y$ by forward substitution. Then we solve 

$$Ux = y$$

for $x$ by backward substitution.

LU factorization also has computational complexity O($n^3$), however the factorization part of 
LU factorization requires half the number of floating point operations compared to Gaussian elimination, 
and forward and backward substitution is O($n^2$)\cite{Kreyszig}.

\subsection{Iterative Methods}

Iterative methods start from an initial guess as to what the correct solution is, 
with the zero vector being a common choice for the initial solution. Then this 
solution is improved by repeating a procedure multiple times, with each step 
giving an improved approximation to the solution of the problem. Iterative methods 
are not guaranteed to produce a correct solution in a given amount of steps. 

Iterative methods stop when a stopping criteria is met, for example when the difference 
between the right-hand side and the left-hand side is small or the change in the approximate 
solution is below a tolerance level given by the user. 

\subsubsection{Jacobi Method}

The Jacobi method works by splitting the matrix $A$ into a diagonal matrix $D$ 
and the lower and upper triangular matrices $L$ and $U$. 

$$A = L+D+U$$

The matrix $D$ is diagonal and contains all the elements on $A$'s diagonal. $L$ 
is lower triangular and contains all the elements from $A$ below the diagonal. 
$U$ is upper triangular and contains all the elements from $A$ above the diagonal.

$$(L+D+U)x = b$$
$$Dx = (b - (L+U)x)$$
$$x = \inv{D}(b - (L+U)x)$$

The system is then solved iteratively by computing 

$$x^{(k+1)} = \inv{D}(b - (L+U)x^{(k)})$$

Written in element form

$$ x_i^{(k+1)} = \frac{1}{a_{ii}} \Big( b_i - \sum_{j \neq i} a_{ij} x_j^{(k)} \Big), ~ i = 1, 2, \ldots, n $$

Because the new value of each $x_i$ is independent of the new value for all the 
other elements in the $x$ vector, the Jacobi method is trivially parallel.

This method converges for every initial guess, if and only if the spectral
radius of $I - A$ is less than 1\cite{Kreyszig}.

\subsubsection{Gauss-Seidel Method}

Gauss-Seidel works similarly to the Jacobi method, by splitting $A$ into $D$, 
$L$ and $U$. The main difference is how the equation is rearranged and the consequences 
when parallelizing the iterative solver. 

$$ (L+D)x = b - Ux $$
$$ (L+D)x^{(k+1)} = b - Ux^{(k)} $$
$$ Dx^{(k+1)} = b - Lx^{(k+1)} - Ux^{(k)} $$
$$ x^{(k+1)} = \inv{D}(b - Lx^{(k+1)} - Ux^{(k)}) $$

The value of $x^{(k+1)}$ has to be computed sequentially using forward substitution. 
There is no room for parallelization. Written in element-based form:

$$ x_i^{(k+1)} = \frac{1}{a_{ii}} \Big( b_i - \sum_{j < i} a_{ij} x_j^{(k+1)} 
- \sum_{j > i} a_{ij} x_j^{(k)} \Big), ~ i = 1, 2, \ldots, n $$

As we see from the first sum, the new value of $x_i$ depends on the new value of 
all previous elements in the $x$ vector.

It is possible to create a parallel implementation of Gauss-Seidel by changing
the order new values of $x$ is calculated. This is known as Red-Black
ordering. The new ordering depends on the linear system.

The Gauss-Seidel method converges if $||C|| < 1$, where $C = -\inv{(I+L)}U$ and 
$||C||$ is some matrix norm of $C$\cite{Kreyszig}.

\subsubsection{Successive Overrelaxation}

Successive overrelaxation (SOR) is a modified version of Gauss-Seidel with the
goal of having faster convergence\cite{Kreyszig}. This method differs from Gauss-Seidel by
introducing a constant $\omega > 1$, which is called the \emph{overrelaxation
factor}. To get the equation for SOR we first add and subtract $x^{(k)}$ on the right 
hand side of Gauss-Seidel.

$$ Dx^{(k+1)} = x^{(k)} + b - Lx^{(k+1)} - (U + I)x^{(k)} $$

Then we introduce the overrelaxation factor $\omega > 1$ to get the SOR formula 
for Gauss-Seidel.

$$ Dx^{(k+1)} = x^{(k)} + \omega \big( b - Lx^{(k+1)} - (U + I)x^{(k)} \big) $$

A recommended value for the overrelaxation factor is $\omega = \frac{2}{1 +
\sqrt{1 - \rho}} $, where $\rho$ is the spectral radius of $-\inv{(I + L)}U$\cite{Kreyszig}. 

\subsection{Krylov Subspace Methods}

The Krylov subspace methods are considered to be some of the most important iterative 
methods for solving linear systems.

In the section on the Jacobi method, the following expression was derived

$$ x^{(k+1)} = \inv{D}(b - (L+U)x^{(k)}) $$

which can be rewritten as 

$$ x^{(k+1)} = (I - \inv{D}A)x^{(k)} + \inv{D}b $$

If $x^{(k)}$ is an approximate solution and $x^{*}$ the exact solution, then the 
$k$th error vector is defined as 

$$ d^{(k)} = x^{(k)} - x^{*}$$

However as the exact solution is unknown, the residual vector is used instead 
to test for convergence. The expression for the $k$th residual vector is

$$ r^{(k)} = b - Ax^{(k)} $$

This expression can be rewritten as

$$ r^{(k)} = b - Ax^{(k)} + x^{(k)} - x^{(k)}$$
$$ r^{(k)} = b + (I - A)x^{(k)} - x^{(k)}$$

Then by normalizing $Ax = b$ by dividing all rows by the value of the diagonal element 
from $A$, or in other words by calculating $\inv{D}Ax = \inv{D}b$, we get a new linear 
system with $I$ as the diagonal matrix, $I = D$. Then replacing $I-A$ with $L+U$ 
leads to the following expression

$$ r^{(k)} = b + (U+L)x^{(k)} - x^{(k)} = x^{(k+1)} - x^{(k)}$$
$$ r^{(k)} = x^{(k+1)} - x^{(k)} $$

We can then rewrite the Jacobi method to

$$ x^{(k+1)} = x^{(k)} + r^{(k)} $$

By multiplying both sides with $-A$ and adding $b$ to both sides, we obtain a 
recursive formula for the residual.

$$ -Ax^{(k+1)} = -Ax^{(k)} - Ar^{(k)} $$
$$ -Ax^{(k+1)} + b = -Ax^{(k)} - Ar^{(k)} + b $$
$$ r^{(k+1)} = r^{(k)} - Ar^{(k)} $$
$$ r^{(k+1)} = (I - A)r^{(k)} $$

From this it follows that 

$$ r^{(n)} = p_n(A)r^{(0)} \in span \{r^{(0)}, Ar^{(0)}, ..., A^{n}r^{(0)}\} $$

where $p_n(A) = (I - A)^n$. From $ x^{(k+1)} = x^{(k)} + r^{(k)} $ we get 

$$ x^{(n)} = x^{(0)} + r^{(0)} + r^{(1)} + ... + r^{(n-1)} = x^{(0)} + q_{n-1}(A)r^{(0)} $$
$$ q_{n-1}(A) = 1 + (I-A) + (I-A)^2 + ... + (I-A)^{n-1} $$

The subspaces that appear in the equation for $r^{(n)}$ and $x^{(n)}$ is called the 
Krylov subspace. The idea behind Krylov subspace solvers is to calculate a sequence 
of approximate solutions of $Ax = b$, so that the residual vectors converge to the 
null vector. If the residuals are linearly independent then 
after a finite number of steps the approximate solution $x_n$ will be equal to the 
exact solution and the residual vector will be zero. This only holds true with exact 
arithmetics, therefore a computer implementation is not guaranteed to reach an exact 
solution\cite{krylovSolvers}.

\subsubsection{Conjugate Gradient Method}

The conjugate gradient method (CG) is a Krylov subspace solver, and is an orthogonal 
projection method. The conjugate gradient method is only guaranteed to work for 
linear systems that are symmetric positive definite.

The matrix $A$ is symmetric positive definite if it is symmetric and $x^T A x$ is 
positive for all non-zero $x$ vectors. Another way of checking if a matrix is 
positive definite is if all it's eigenvalues are positive. 

The conjugate gradient method attempts to find the value of the $x$ vector which 
minimizes the following function

$$ \Psi(x) = \frac{1}{2}x^T A x - b^T x + \gamma $$

If $A$ is symmetric positive definite, then $\Psi$ is convex which means it has 
a unique minimum (no local minimums). Its gradient is 

$$ \grad \Psi(x) = Ax-b = -r $$

Here $r$ is the residual vector. Minimizing $\Psi(x)$ is the same as finding 
$\grad \Psi(x) = 0$, which is the same as solving $Ax = b$.
