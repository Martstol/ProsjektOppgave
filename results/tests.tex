\section{Tests}

TODO: Write about all the snow simulator configurations when doing these tests
Also write about what solvers were used by PETSc and which solver is used
by the default gpu snow simulator

TODO: When running the solver on the cpu the following command line arguments
are given: "\emph{-vec\_type standard -mat\_type aij}" and on the gpu:
"\emph{-vec\_type viennacl -mat\_type aijviennacl}".

The configuration options for the wind simulator allows for variation in the
external wind direction, speed and the resolution of the three dimensional wind
velocity field. For the scope of these tests only the resolution of the wind
velocity field is changed, while the external wind's speed and direction remains
unchanged for all tests. The resolution of the wind velocity field is denoted
as \{x, y, z\}. The terrain used for the simulation tests is the height map of
Mount St. Helens with a size of 768x768. The external wind's speed is set to 1
and the direction is given by the unit vector \{1, 0, 0\}.

\subsection{Convergence Tests}

TODO: Write about the convergence

\subsection{Performance Tests}

\subsubsection{Time Distribution}

This test measures how the execution time of the wind simulation is distributed
between these key functions in the implementation:
\begin{description}
	\item[advect:] Performs the advection step of the wind simulation.
	\item[setupSolution:] Computes the right-hand side of the linear system.
	\item[setInitialGuess:] Sets the initial guess for the solver.
	\item[solve:] Solves the Poisson equation using a PETSc solver.
	\item[project:] Performs the projection step of the wind simulation.
	\item[windToGPU:] Moves the wind velocity field to texture memory for the
	snow particle simulation.
\end{description}

The solver chosen for this test is GMRES, this was specified with the command
line argument "\emph{-ksp\_type gmres}".

This test is performed for four different configurations for the resolution of
the wind velocity field:
\begin{description}
	\item[Configuration 1] $ \{ 32, 16, 32 \} $, results are shown in figure
		\ref{fig:td_conf1}.
	\item[Configuration 2] $ \{ 64, 32, 64 \} $, results are shown in figure
		\ref{fig:td_conf2}.
	\item[Configuration 3] $ \{ 128, 64, 128 \} $, results are shown in figure
		\ref{fig:td_conf3}.
	\item[Configuration 4] $ \{ 256, 128, 256 \} $, results are shown in figure
		\ref{fig:td_conf4}.
\end{description}

The results for the time distribution test is the average execution time of each
key function after 100 frames of simulation.

\begin{figure}[ht]
	\center
	
	\begin{subfigure}{0.45\textwidth}
		\center
		\includegraphics[width=1.0\textwidth]{results/data/td/td_conf1_petsc_gpu}
		\caption{PETSc on the GPU with OpenCL.}
		\label{fig:td_conf1_petsc_gpu}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\center
		\includegraphics[width=1.0\textwidth]{results/data/td/td_conf1_petsc_cpu}
		\caption{PETSc on the CPU.}
		\label{fig:td_conf1_petsc_cpu}
	\end{subfigure}
	\caption{Time distribution of the execution time of the key functions
			with configuration 1}
	\label{fig:td_conf1}
	
\end{figure}

\begin{figure}[ht]
	\center
	
	\begin{subfigure}{0.45\textwidth}
		\center
		\includegraphics[width=1.0\textwidth]{results/data/td/td_conf2_petsc_gpu}
		\caption{PETSc on the GPU with OpenCL.}
		\label{fig:td_conf2_petsc_gpu}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\center
		\includegraphics[width=1.0\textwidth]{results/data/td/td_conf2_petsc_cpu}
		\caption{PETSc on the CPU.}
		\label{fig:td_conf2_petsc_cpu}
	\end{subfigure}
	\caption{Time distribution of the execution time of the key functions
			with configuration 2}
	\label{fig:td_conf2}
	
\end{figure}

\begin{figure}[ht]
	\center
	
	\begin{subfigure}{0.45\textwidth}
		\center
		\includegraphics[width=1.0\textwidth]{results/data/td/td_conf3_petsc_gpu}
		\caption{PETSc on the GPU with OpenCL.}
		\label{fig:td_conf3_petsc_gpu}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\center
		\includegraphics[width=1.0\textwidth]{results/data/td/td_conf3_petsc_cpu}
		\caption{PETSc on the CPU.}
		\label{fig:td_conf3_petsc_cpu}
	\end{subfigure}
	\caption{Time distribution of the execution time of the key functions
			with configuration 3}
	\label{fig:td_conf3}
	
\end{figure}

\begin{figure}[ht]
	\center
	
	\begin{subfigure}{0.45\textwidth}
		\center
		\includegraphics[width=1.0\textwidth]{results/data/td/td_conf4_petsc_gpu}
		\caption{PETSc on the GPU with OpenCL.}
		\label{fig:td_conf4_petsc_gpu}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\center
		\includegraphics[width=1.0\textwidth]{results/data/td/td_conf4_petsc_cpu}
		\caption{PETSc on the CPU.}
		\label{fig:td_conf4_petsc_cpu}
	\end{subfigure}
	\caption{Time distribution of the execution time of the key functions
			with configuration 4}
	\label{fig:td_conf4}
	
\end{figure}

As evident from these charts we can see that as the resolution of the wind
velocity field increases, the fraction of the wind simulation time spent solving
the Poisson equation remains roughly unchanged for the PETSc on the CPU only
implementation, however when the GPU is utilized for the solver the fraction of
the simulation time spent solving the Poisson equation is reduced rapidly as
the computational power of the GPU is better utilized for larger problems because
of the GPU's parallel nature.

\subsubsection{Wind Simulator Benchmark}

% Execution time | number of internal points

\subsubsection{Solver Benchmark}

% Execution time | number of internal points

% Solvers: GMRES GPU, CG GPU, GMRES CPU, CG CPU, CUDA old
% Dimensions: {32, 16, 32}, {32, 32, 32}, {64, 32, 64}, {64, 64, 64}
% {128, 64, 128}, {128, 128, 128}, {256, 128, 256}

Here the execution time of the Poisson solver is benchmarked. The benchmark
measures the execution time of the solver as a function of the size of the
wind field. The execution time of the solver used for the benchmark is the
average execution time after 100 frames of simulation. The following
configurations for the resolution of the wind field was used for this benchmark:
\begin{itemize}
	\item \{32, 16, 32\}
	\item \{32, 32, 32\}
	\item \{64, 32, 64\}
	\item \{64, 64, 64\}
	\item \{128, 64, 128\}
	\item \{128, 128, 128\}
	\item \{256, 128, 256\}
\end{itemize}
The total size of the wind field includes the external boundary points.
The solver methods selected for the benchmark is the conjugate gradient method
(CG) and general minimum residual method (GMRES). Both the CPU and the GPU
implementation of these methods are included in the benchmark. In addition the
SOR solver implemented in the wind simulator has been included in the benchmark.
The SOR solver have been hand implemented in CUDA during previous work on the
snow simulator performed by other students, the previous work on the snow
simulator is discussed in chapter \ref{chap:prevwork}. The purpose of including
the SOR solver is to compare its performance with the PETSc solvers.

\begin{figure}[ht]
	\center
	\includegraphics[width=1.0\textwidth]{results/data/sb/exec_time_all}
	\caption{Graph comparing execution time of the different solvers for the
		wind simulator.}
	\label{fig:sb_exec_time_all}
\end{figure}

\subsection{Visual Results}

TODO: Show the windfield and pressure field and obstacle field (if I have time)
